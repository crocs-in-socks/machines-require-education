{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION : \n",
    "\n",
    "This model solves the classification problem, that is, it classifies data points into discrete categories. In binary classification, we call the false category the 'negative' class and the true category the 'positive' class.\n",
    "\n",
    "Logistic regression specifically solves binary classification problems. Logistic regression fits an s-shaped curve (sigmoid function) to our data, and hence creates a classification boundary based on which we can classify data points.\n",
    "\n",
    "The sigmoid function is represented by : \n",
    "\n",
    "$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}, where : 0 < g(z) < 1\n",
    "$\n",
    "\n",
    "at z = 0, g(z) = 0.5\n",
    "\n",
    "For logistic regression, we let z = w.x + b, and put z in the sigmoid function, effectively getting a model that uses the input features and outputs a value between 1 and 0.\n",
    "\n",
    "$\n",
    "f_{w, b}(x) = g(w.x + b) = \\frac{1}{1 + e^{-(w.x + b)}}\n",
    "$\n",
    "\n",
    "The model's function basically outputs the probability that the class is 1, given a certain input x.\n",
    "\n",
    "Now, the model outputs a number between 0 and 1, but we want it to decide which it is, 0 or 1. So, we may decide that above a certain number, we take the output as 1 and below this number we take the output as 0.\n",
    "\n",
    "This number is generally 0.5, but does not have to be. From this number we can see for what values of z the model with output 1 and for what values of z the model will output 0. This boundary created is called the decision boundary.\n",
    "\n",
    "The shape of the decision boundary depends on the function we take as z.\n",
    "\n",
    "COST FUNCTION :\n",
    "\n",
    "Here the squared error cost function is not an ideal cost function, so we take a look at a different cost function.\n",
    "\n",
    "If we tried to plot the squared error cost function for the logistic regression function, we would get a non-convex curve and this might result in getting stuck at a local minima.\n",
    "\n",
    "The squared error cost function is : \n",
    "\n",
    "$\n",
    "J_{w, b} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}\\frac{1}{2}(f_{w, b}(x^{(i)}) - y^{(i)})^{2}\n",
    "$\n",
    "\n",
    "Let us take the term inside the summation as the loss function. $L(f_{w, b}(x^{(i)}), y^{(i)})$\n",
    "\n",
    "In logistic regression, we take the loss function as : \n",
    "\n",
    "$\n",
    "L(f_{w, b}(x^{(i)}), y^{(i)}) = -log(f_{w, b}(x^{(i)})) $\n",
    "if $y^{(i)} = 1$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "L(f_{w, b}(x^{(i)}), y^{(i)}) = -log(1 - f_{w, b}(x^{(i)}))$  if $y^{(i)} = 0$\n",
    "\n",
    "With this loss function, the cost function is now convex and hence we can reliably reach a global minimum. Hence, the cost function of logistic regression is given as : \n",
    "\n",
    "$\n",
    "J_{w, b} = \\frac{1}{m}\\sum\\limits_{i=1}^{m}L(f_{w, b}(x^{(i)}), y^{(i)})\n",
    "$\n",
    "\n",
    "Where the loss function is as defined above. The simplified loss function is as follows : \n",
    "\n",
    "$\n",
    "L(f_{w, b}(x^{(i)}), y^{(i)}) = -y^{(i)}log(f_{w, b}(x^{(i)})) - (1 - y^{(i)})log(1 - f_{w, b}(x^{(i)}))\n",
    "$\n",
    "\n",
    "This loss function was obtained by using a statistical principle called maximum likelihood estimation.\n",
    "\n",
    "GRADIENT DESCENT : \n",
    "\n",
    "On applying the formulae for gradient descent, we get the updated values for w and b as : \n",
    "\n",
    "$\n",
    "w_{j} = w_{j} - \\alpha * \\frac{1}{m}\\sum\\limits_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})x_{j}^{(i)}\n",
    "$\n",
    "\n",
    "$\n",
    "b = b - \\alpha * \\frac{1}{m}\\sum\\limits_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})\n",
    "$\n",
    "\n",
    "These functions are in fact very similar to the formulae we wrote for linear regression, except f(x) is different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGULARIZATION :\n",
    "\n",
    "This is a method to reduce the impact of a feature without outright eliminating it and is used to address overfitting. This is generally done by reducing the weight of the feature in the model's function.\n",
    "\n",
    "We can do this by adding more terms to the cost function. For example :\n",
    "\n",
    "$\n",
    "J_{w, b} = \\frac{1}{2m}\\sum\\limits_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})^{2} + 1000w_{3}^{2} + 1000w_{4}^{2}\n",
    "$\n",
    "\n",
    "Here the only way to minimize J would be to make $w_{3}$ and $w_{4}$ small, as if they had larger values, the cost would become much larger. Thus on minimizing the above function, we get $w_{3}$ and $w_{4}$ close to zero, effectively nearly cancelling the effect of the termas these weights are for.\n",
    "\n",
    "However, sometimes we don't know which features are the ones we want to regularize. So, we just regularize all of them by adding the term $\\lambda$ to the cost function.\n",
    "\n",
    "$\n",
    "J_{w, b} = \\frac{1}{2m}\\sum\\limits_{i=1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})^{2} + \\frac{\\lambda}{2m}\\sum\\limits_{j=1}^{m}w_{j}^{2}\n",
    "$\n",
    "\n",
    "Here $\\lambda$ is called the regularization parameter, and this is arbitrarily set. We've divided $\\lambda$ by 2m because when both terms of the cost function are scaled by the same value, it becomes easier to choose values for $\\lambda$.\n",
    "\n",
    "By convention, we do not penalize b for being large.\n",
    "\n",
    "So, in this new cost function we try to minimize both the mean sqaured error as well as the regularization term. If $\\lambda$ is zero, we aren't regularizing at all, and if $\\lambda$ is a very large number, we are setting all the weights to almost zero (not good), causing the model to underfit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization term in the above cost function is the same for both linear and logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
